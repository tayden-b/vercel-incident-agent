import { db } from './db';

export interface AnalysisResult {
    summary: string;
    likely_causes: Array<{ cause: string; confidence: number; evidence: string }>;
    recommended_action: 'redeploy' | 'investigate' | 'ignore';
    next_steps: string[];
    risk_notes: string[];
}

export async function analyzeIncident(signature: string, evidence: string[]): Promise<AnalysisResult> {
    const apiKey = process.env.LLM_API_KEY;
    const baseUrl = process.env.LLM_BASE_URL || 'https://api.openai.com/v1';
    const model = process.env.LLM_MODEL || 'gpt-4o-mini';

    // Check cache
    const cached = await db.analysis.findUnique({
        where: { errorSignature: signature },
    });

    if (cached) {
        return {
            summary: cached.summary,
            likely_causes: JSON.parse(cached.likelyCausesJson),
            recommended_action: cached.recommendedAction as any,
            next_steps: JSON.parse(cached.nextStepsJson),
            risk_notes: [], // Cache doesn't store this in MVP schema but could be added
        };
    }

    if (!apiKey) {
        // LLM Disabled Fallback
        return {
            summary: "LLM analysis is disabled (missing API key). Manual investigation required.",
            likely_causes: [{ cause: "Unknown", confidence: 1, evidence: "N/A" }],
            recommended_action: "investigate",
            next_steps: ["Check logs manually", "Verify environment variables"],
            risk_notes: ["Analysis not generated by LLM"],
        };
    }

    const prompt = `
Analyze the following runtime error logs for a Vercel project and provide a JSON report.

Error Details:
${evidence.join('\n').slice(0, 4000)}

Output JSON strictly:
{
  "summary": "Short 1-2 sentence summary of the issue",
  "likely_causes": [
    {"cause": "Description of cause", "confidence": 0-1, "evidence": "Snippet from logs"}
  ],
  "recommended_action": "redeploy|investigate|ignore",
  "next_steps": ["Step 1", "Step 2"],
  "risk_notes": ["Any risks associated with the action"]
}
`;

    try {
        const response = await fetch(`${baseUrl}/chat/completions`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${apiKey}`,
            },
            body: JSON.stringify({
                model,
                messages: [{ role: 'user', content: prompt }],
                response_format: { type: 'json_object' },
            }),
        });

        if (!response.ok) {
            throw new Error(`LLM API error: ${await response.text()}`);
        }

        const data = await response.json();
        const result: AnalysisResult = JSON.parse(data.choices[0].message.content);

        // Cache the result
        await db.analysis.create({
            data: {
                errorSignature: signature,
                summary: result.summary,
                likelyCausesJson: JSON.stringify(result.likely_causes),
                recommendedAction: result.recommended_action,
                nextStepsJson: JSON.stringify(result.next_steps),
                modelUsed: model,
            },
        });

        return result;
    } catch (error: any) {
        console.error('LLM Analysis failed:', error);
        return {
            summary: "LLM analysis failed due to an error.",
            likely_causes: [],
            recommended_action: "investigate",
            next_steps: ["Check LLM API status"],
            risk_notes: [String(error)],
        };
    }
}
